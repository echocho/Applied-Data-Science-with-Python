{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Topic Modeling\n",
    "\n",
    "## Semantic Text Similarity\n",
    "### Applications of semantic similarity\n",
    "1. Grouping similar words into semantic concepts\n",
    "2. As a building block in natural language understanding tasks\n",
    "    - Textual entailment\n",
    "    - Paraphrasing\n",
    "\n",
    "### WordNet\n",
    "1. Semantic dictionary of (mostly) English words, interlinked by semantic relations\n",
    "2. Includes rich linguistic information\n",
    "    - part of speech, word senses, synonyms, hypernyms, meronyms, derivationally related forms, ...\n",
    "3. Machine-readable, freely available\n",
    "\n",
    "### Semantic similary using WordNet\n",
    "1. Wordnet organizes information in a hierarchy\n",
    "2. Many similarity measures use the hierarchy in some way\n",
    "3. Verbs, nouns, adjectives all have separate hierarchies\n",
    "For example,\n",
    "<img src=\"https://img.ceclinux.org/39/686ed8dc683f3a7218cf38af0063965d493d87.png\">\n",
    "\n",
    "### Path Similarity\n",
    "1. Find the shortest path between the two concepts\n",
    "2. Similarity measure inversely related to path distance\n",
    "    - PathSim(deer, elk) = 1/1+distance = 1/(1+1) = 0.5\n",
    "    - PathSim(deer, giraffe) = 1/(1+1+1) = 0.33\n",
    "    - PathSim(deer, horse) = 1/7 = 0.14 \n",
    "\n",
    "### Lowest Common Subsumer (LCS)\n",
    "1. Find the closest ancestor to both concepts\n",
    "    - LCS(deer, elk) = deer\n",
    "    - LCS(deer, giraffe) = ruminant\n",
    "    - LCS(deer, horse) = ungulate\n",
    "    \n",
    "### Lin Similarity\n",
    "1. Similarity measure based on the information contained in the LCS of the two concepts\n",
    "    - LinSim(u,v) = 2 * logP(LCS(u,v)) /logP(u) + logP(V))\n",
    "2. P(u) is given by the information content learnt over a large corpus\n",
    "\n",
    "``` python3\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# find appropriate sense of the words\n",
    "deer = wn.synset('deer.n.01')\n",
    "elk = wn.synset('elk.n.01')\n",
    "\n",
    "# find path similarity\n",
    "deer.path_similarity(elk)\n",
    "deer.path_similarity(horse)\n",
    "```\n",
    "\n",
    "```python3\n",
    "# use an infromation criteria to find Lin Similarity\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "deer.lin_similarity(elk, brown_ic)\n",
    "deer.lin_similarity(horse, brown_ic)\n",
    "```\n",
    "\n",
    "### Collcations and Disributional similarity\n",
    "1. \"You know a word by the company it keeps\"\n",
    "2. Two words that frequently appear in similar contexts are more likely to be semantically related. \n",
    "    e.g. \n",
    "    - The friends met at a cafe (met, at, a)\n",
    "    - Shayne met Ray at a pizzeria (met, at, a)\n",
    "\n",
    "### Distributional Similarity: Context\n",
    "1. Words before, after, within a small window\n",
    "2. Parts of speech of words before, after, in a small window\n",
    "3. Specific syntactic relation to the target word\n",
    "4. Words in the same sentence, same document, ..\n",
    "\n",
    "### Strength of association between words\n",
    "1. Not similar if two words don't occur together often\n",
    "2. Also important to see how frequent are individual words\n",
    "    - e.g. \"the\"\n",
    "3. Pointwise Mutual Information \n",
    "    **PMI(w,c) = log[P(w,c)/P(w)P(c)]**\n",
    "\n",
    "```python3\n",
    "# use NLTK collocations and association measures\n",
    "import nltk\n",
    "from nltk.collections import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(text)\n",
    "finder.nbest(bigram_measrues.pmi, 10)\n",
    "\n",
    "# finder also has other useful funcions, e.g. frequency filter\n",
    "finder.apply_freq_filter(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "1. A coarse-level analysis of what's in a text collection\n",
    "2. Topic: the subject (theme) of a discourse\n",
    "3. Topics are represented as a word distribution\n",
    "4. A document is assumed to be a mixture of topics\n",
    "5. What's known: \n",
    "    - the text collection or corpus\n",
    "    - number of topics\n",
    "6. What's not known:\n",
    "    - the actual topics\n",
    "    - topic distribution for each document\n",
    "7. Essentially, topic modeling is a text clustering problem\n",
    "    - Documents and words clustered simultaneously\n",
    "8. Different topic modeling approaches available\n",
    "    - Probabilistic Latent Semantic Analysis (PLSA) (Hoffman 99)\n",
    "    - Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models and LDA\n",
    "1. Generative models: unique text model; mixture model\n",
    "<img src=\"https://img.ceclinux.org/7b/861668889570f23a0f052631a954a4e0eb8c56.png\">\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "(A very simplistic explanation) \n",
    "Generative model for a document d\n",
    "    - choose the length of document d\n",
    "    - choose a mixture of topics for document d\n",
    "    - use a topic's multinomial distribution to output words to fill that topic's quota\n",
    "    \n",
    "### Topic Modeling in Practice \n",
    "1. How many topics\n",
    "    - finding or even guessing the number of topics is hard\n",
    "2. Interpreting topics\n",
    "    - topics are just word distributions\n",
    "    - making sense of words/ generating labels is subjective\n",
    "\n",
    "### Topic Modeling in Summary\n",
    "1. Great tool for exploratory text analysis\n",
    "    - Helps answering the Q: what are the documents (tweets, reviews, news, articles) about?\n",
    "2. Many tools availabe to do it fairly effortlessly in Python\n",
    "    \n",
    "### Working with LDA in python\n",
    "1. packages, e.g. gensim Ida\n",
    "2. Pre-processing text\n",
    "    - tokenize, normalize (lowercase)\n",
    "    - stop word removal\n",
    "    - stemming\n",
    "3. Convert tokenized documents to a document - term matrix\n",
    "4. Build LDA models on the doc-term matrix\n",
    "5. (example) doc_set: set of pre-processed text documents\n",
    "    ```python3\n",
    "    import gensim\n",
    "    from gensim import corpora, models\n",
    "    \n",
    "    dictionary = corpora.Dictionary(doc_set)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in doc_set]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=50)\n",
    "    \n",
    "    print(ldamodel.print_topics(num_topics=4, num_words=5))\n",
    "    ```\n",
    "6. ldamodel can also be used to find topic distribution of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
