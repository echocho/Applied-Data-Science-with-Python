{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Text\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "### Examples of Text Classification\n",
    "1. Topic identification\n",
    "2. Sapam dection\n",
    "3. Sentiment analysis: is this movie review positive or negative \n",
    "4. Spelling correction: weather or whether? color or colour?\n",
    "\n",
    "### Supervised Classification\n",
    "1. Learn a **classification model** on properties ('features') and their importance ('weights') from labeled instances \n",
    "2. Phases: training phase; inference phase\n",
    "3. Datasets: training date set; validation data set; test data set\n",
    "\n",
    "### Classification Paradigms\n",
    "1. When there are only two possible classes; |Y| = 2:\n",
    "**Binary Classification**\n",
    "2. When there are only two possible classes; |Y| > 2:\n",
    "**Multi-class Classification**\n",
    "3. When data instances can have two or more labels:\n",
    "**Multi-label Classification**\n",
    "\n",
    "### Qs to ask in Supervised Learning\n",
    "1. Training phase\n",
    "    - what are the features? how do you represent them?\n",
    "    - what is the classification model /algorithm?\n",
    "    - what are the model parameters?\n",
    "2. Inference phase\n",
    "    - what the is expected performance? what is a good measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Features from Text\n",
    "\n",
    "### Why is texutal data unique?\n",
    "1. Textual data presents a unique set of challenges\n",
    "2. All the information you need is in text\n",
    "3. But features can be pulled out from text at different granularities.\n",
    "\n",
    "### Types of textural features\n",
    "1. Words\n",
    "    - by far the most common class of features\n",
    "    - handling commonly-occurring words: Stop Words\n",
    "    - normalization: make lower case vs. leave as-is\n",
    "    - stemming /lemmatization\n",
    "2. Characteristics of words\n",
    "    - capitalization\n",
    "    - parts of speech of words in a sentence\n",
    "    - grammatical structure, sentence parsing\n",
    "    - grouping words of similar meaning, semantics\n",
    "        - {buy, purchase}\n",
    "        - {Mr., Ms., Dr., Prof.}; \n",
    "        - numbers/digits\n",
    "        - dates\n",
    "3. Depending on classification tasks, features may come from inside words and word sequnces\n",
    "    - bigrams, trigrams, n-grams; \"White House\"\n",
    "    - character sub-sequences in words: 'ing', 'ion', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers\n",
    "\n",
    "### Case Study: Classifing text search queries\n",
    "1. You are interested in classfiying serach queries in 3 classes: Entertainment, Computer Science, Zoology. And most common class of the three is Entertainment.\n",
    "2. If the query is \"python\", it could be either on of the class. For example, 1) python the snake; 2) python the programming language; 3) Monte Python. But in most class, given \"python\", is Zoology.\n",
    "3. If the query is \"python download\". Most probable class, given \"python download\", is Computer Science.\n",
    "\n",
    "### Probabilistic Model\n",
    "1. In the example above, a probabilistic model tells you the likehood of a class before you have any information.\n",
    "2. And you update the likelihood fo the class given new information.\n",
    "3. Prior Probability: Pr(y=Entertainment), Pr(y=CS), Pr(y=Zoology)\n",
    "4. Posterior Probability: Pr(y=Zoology|x='python')\n",
    "\n",
    "\n",
    "### Bayes' Rule\n",
    "1. $ Posterior\\ probability = \\frac {{Prior\\ Probability} \\times  {Likelihood}}{Evidence} $\n",
    "2. $ Pr(y|X) = \\frac {Pr(y) \\times Pr(X|y)}{(Pr(X))} $\n",
    "3. $ Pr(CS|\"python\") = \\frac {Pr(y=CS) \\times Pr(\"python\"|y=CS)}{(Pr(\"python\"))} $\n",
    "\n",
    "### Naive Bayes Classification\n",
    "1. Naive assumption: given the class label, features are assumed to be independent of each other.\n",
    "2. Formulation \n",
    "<img src=\"https://img.ceclinux.org/eb/d3d44d2259c613b95a0d373805245fc41c5809.png\">\n",
    "3. Example\n",
    "<img src=\"https://img.ceclinux.org/21/b3eee56e85c2377062853774f9055e17135e21.png\">\n",
    "\n",
    "\n",
    "### Naive Bayes: Parameters\n",
    "1. Prior probabilities: Pr(y) for all y in Y\n",
    "2. Likelihood: $Pr(x_{i}|y)$ for all features $x_i$ and labels y in Y\n",
    "3. If there are 3 classes (|y|=3) and 100 features in X, how many parameters does naive Bayes models have?\n",
    "4. Ans: |Y| + 2 x X x |Y|\n",
    "\n",
    "### Naive Bayes: Smoothing\n",
    "1. If feature $x_i$ never occurs in documents labeled y, $Pr(y|x_i)$ will be 0\n",
    "2. Should smooth the parameters\n",
    "    - **laplace smooth** or **additive smoothing**: add a dummy counts, e.g. add count 1 to all features\n",
    "    \n",
    "### Summary \n",
    "1. Naive Bayes is a probabilistic model\n",
    "2. It assumes features are independent of each other, given the class label\n",
    "3. Not necessarily true, even for text mining\n",
    "4. For text classificatino problems, naive Bayes usually provides very strong baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Variations\n",
    "### 2 Classic Naive Bayes Variants for Text\n",
    "1. Multinomial Naive Bayes\n",
    "    - Assumption: Data follows a multinomial distribution\n",
    "    - Each feature is a count (word occurrence counts, TF-IDF weighting, ...)\n",
    "        - TF: term frequency\n",
    "        - IDF: inverse document frequence (both adding more frequency/importance to some words)\n",
    "2. Bernoulli Naive Bayes\n",
    "    - Assumption: Data follows a multivariate Bernoulli distribution\n",
    "    - Each feature is binary (e.g. word is present/ absent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "1. Classifier = Function on input data\n",
    "2. Decision Boundaries\n",
    "    - a classification function is represented by decision surfaces\n",
    "    - <img src=\"https://img.ceclinux.org/d3/8440af9c80b446b33144ef4c28a7ac874e705f.png\">\n",
    "    - Linear boundaries\n",
    "        - easy to find; easy to evaluate\n",
    "        - more generalizable\n",
    "        - Maximum Margin\n",
    "        <img src=\"https://img.ceclinux.org/b4/bf2205c52fa4846d8ca60ead603c167ddcdc24.png\">\n",
    "        - **Support Vector Machines** are maximum-margin classifiers\n",
    "        <img src=\"https://img.ceclinux.org/72/214124d871334fc95a3ce579c192c280ebf8a6.png\">\n",
    "        \n",
    "### Support Vector Machines (SVM)\n",
    "1. SVMs are **linear classifiers** that find a hyperplane to separate **two classes** of data: positive and negative \n",
    "2. Given training data $(x_1,y_1)$, $(x_2,y_2)$, etc where $x_i = (x_1, x_2,...x_n)$ is instance vector and $y_i$ is one of {-1, +1}\n",
    "3. SVM finds a linear function w (weight vector) \n",
    "\n",
    "    $f(x_i) = <w.x_i> + b$\n",
    "    \n",
    "    if $f(x_i) >= 0, y_i = +1$, else $y_i = -1$\n",
    "\n",
    "###  SVM: Multi-class classification\n",
    "1. One vs. Rest\n",
    "    - n-class SVM has n classifiers\n",
    "    - <img src=\"https://img.ceclinux.org/ea/57365ec7e7e8c669b8bef83082f4d19597decf.png\">\n",
    "2. One vs. One\n",
    "    - n-class SVM has C(n,2) classifiers\n",
    "    <img src=\"https://img.ceclinux.org/26/9ab9469893c2d4e8593edeb414fe8a6464522e.png\">\n",
    "\n",
    "### SVM Parameters\n",
    "1. Parameter C\n",
    "    - regularization: how uch importance should you give individual data points as compared to better generalized model\n",
    "    - larger c = less regularization\n",
    "         - fit training data as well as possible, every data point is important\n",
    "2. Other params\n",
    "     - Linear kernels usually work best for text data\n",
    "         - other kernels include rbf, polynomial\n",
    "     - multi_class: ovr (one-vs-rest)\n",
    "     - class_weight: different calsses can get different weights\n",
    "     \n",
    "### Summary\n",
    "1. SVMs tend to be the most accurate classifiers, esp. n high-dimensional data\n",
    "2. Strong thoretical foundation (optimization theory)\n",
    "3. Handle only numeric features\n",
    "    - convert categorical features to numeric features\n",
    "    - normalization needed (all in 0-1 range)\n",
    "4. Hyperplane hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Text Classifiers in Python\n",
    "### Model Selection in Scikit-learn\n",
    "```\n",
    "from sklearn import model_selection\n",
    "\n",
    "# normal split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(train_data, train_lables, test_size = 0.333, random_state=0)\n",
    "\n",
    "# 5 fold cross validation\n",
    "predicted_labels = model_selection.cross_val_predict(clfrSVM, train_data, train_labels, cv=5)\n",
    "```\n",
    "\n",
    "### Supervised Text Classification in NLTK\n",
    "Classification algorigthms in NLTK:\n",
    "    - NaiveBayesCalssifier\n",
    "    - DecisionTreeClassifier\n",
    "    - ConditionalExponentialClassifier\n",
    "    - MaxentClassifier\n",
    "    - WekaClassifier\n",
    "    - SklearnClassifier\n",
    "\n",
    "### Using NLTK's NaiveBayesClassifiers\n",
    "Example: Naive Bayes\n",
    "```\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "classifier.classify(unlabeled_instance)\n",
    "classifier.classify_many(unlabeled_istances)\n",
    "\n",
    "nltk.classify.util.accuracy(classifier, test_set)\n",
    "\n",
    "classifier.labels()\n",
    "\n",
    "classifier.show_most_informative_features()\n",
    "```\n",
    "\n",
    "Example: SVM\n",
    "``` \n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfnb = SklearnClassifier(MultinomialNB()).train(train_set)\n",
    "\n",
    "clfsvm = SklearnClassifier(SVC()), kernel='linear').train(train_set)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
