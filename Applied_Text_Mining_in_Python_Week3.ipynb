{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Text\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "### Examples of Text Classification\n",
    "1. Topic identification\n",
    "2. Sapam dection\n",
    "3. Sentiment analysis: is this movie review positive or negative \n",
    "4. Spelling correction: weather or whether? color or colour?\n",
    "\n",
    "### Supervised Classification\n",
    "1. Learn a **classification model** on properties ('features') and their importance ('weights') from labeled instances \n",
    "2. Phases: training phase; inference phase\n",
    "3. Datasets: training date set; validation data set; test data set\n",
    "\n",
    "### Classification Paradigms\n",
    "1. When there are only two possible classes; |Y| = 2:\n",
    "**Binary Classification**\n",
    "2. When there are only two possible classes; |Y| > 2:\n",
    "**Multi-class Classification**\n",
    "3. When data instances can have two or more labels:\n",
    "**Multi-label Classification**\n",
    "\n",
    "### Qs to ask in Supervised Learning\n",
    "1. Training phase\n",
    "    - what are the features? how do you represent them?\n",
    "    - what is the classification model /algorithm?\n",
    "    - what are the model parameters?\n",
    "2. Inference phase\n",
    "    - what the is expected performance? what is a good measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Features from Text\n",
    "\n",
    "### Why is texutal data unique?\n",
    "1. Textual data presents a unique set of challenges\n",
    "2. All the information you need is in text\n",
    "3. But features can be pulled out from text at different granularities.\n",
    "\n",
    "### Types of textural features\n",
    "1. Words\n",
    "    - by far the most common class of features\n",
    "    - handling commonly-occurring words: Stop Words\n",
    "    - normalization: make lower case vs. leave as-is\n",
    "    - stemming /lemmatization\n",
    "2. Characteristics of words\n",
    "    - capitalization\n",
    "    - parts of speech of words in a sentence\n",
    "    - grammatical structure, sentence parsing\n",
    "    - grouping words of similar meaning, semantics\n",
    "        - {buy, purchase}\n",
    "        - {Mr., Ms., Dr., Prof.}; \n",
    "        - numbers/digits\n",
    "        - dates\n",
    "3. Depending on classification tasks, features may come from inside words and word sequnces\n",
    "    - bigrams, trigrams, n-grams; \"White House\"\n",
    "    - character sub-sequences in words: 'ing', 'ion', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers\n",
    "\n",
    "### Case Study: Classifing text search queries\n",
    "1. You are interested in classfiying serach queries in 3 classes: Entertainment, Computer Science, Zoology. And most common class of the three is Entertainment.\n",
    "2. If the query is \"python\", it could be either on of the class. For example, 1) python the snake; 2) python the programming language; 3) Monte Python. But in most class, given \"python\", is Zoology.\n",
    "3. If the query is \"python download\". Most probable class, given \"python download\", is Computer Science.\n",
    "\n",
    "### Probabilistic Model\n",
    "1. In the example above, a probabilistic model tells you the likehood of a class before you have any information.\n",
    "2. And you update the likelihood fo the class given new information.\n",
    "3. Prior Probability: Pr(y=Entertainment), Pr(y=CS), Pr(y=Zoology)\n",
    "4. Posterior Probability: Pr(y=Zoology|x='python')\n",
    "\n",
    "\n",
    "### Bayes' Rule\n",
    "1. $ Posterior\\ probability = \\frac {{Prior\\ Probability} \\times  {Likelihood}}{Evidence} $\n",
    "2. $ Pr(y|X) = \\frac {Pr(y) \\times Pr(X|y)}{(Pr(X))} $\n",
    "3. $ Pr(CS|\"python\") = \\frac {Pr(y=CS) \\times Pr(\"python\"|y=CS)}{(Pr(\"python\"))} $\n",
    "\n",
    "### Naive Bayes Classification\n",
    "1. Naive assumption: given the class label, features are assumed to be independent of each other.\n",
    "2. Formulation \n",
    "<img src=\"https://img.ceclinux.org/eb/d3d44d2259c613b95a0d373805245fc41c5809.png\">\n",
    "3. Example\n",
    "<img src=\"https://img.ceclinux.org/21/b3eee56e85c2377062853774f9055e17135e21.png\">\n",
    "\n",
    "\n",
    "### Naive Bayes: Parameters\n",
    "1. Prior probabilities: Pr(y) for all y in Y\n",
    "2. Likelihood: $Pr(x_{i}|y)$ for all features $x_i$ and labels y in Y\n",
    "3. If there are 3 classes (|y|=3) and 100 features in X, how many parameters does naive Bayes models have?\n",
    "4. Ans: |Y| + 2 x X x |Y|\n",
    "\n",
    "### Naive Bayes: Smoothing\n",
    "1. If feature $x_i$ never occurs in documents labeled y, $Pr{y|x_i)$ will be 0\n",
    "2. Should smooth the parameters\n",
    "    - **laplace smooth** or **additive smoothing**: add a dummy counts, e.g. add count 1 to all features\n",
    "    \n",
    "### Summary \n",
    "1. Naive Bayes is a probabilistic model\n",
    "2. It assumes features are independent of each other, given the class label\n",
    "3. Not necessarily true, even for text mining\n",
    "4. For text classificatino problems, naive Bayes usually provides very strong baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
