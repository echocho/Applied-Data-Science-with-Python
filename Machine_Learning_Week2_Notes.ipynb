{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Supervised Maching Learning\n",
    "`A more detailed and lower level ML course suggested: by Andrew Ng, also on Coursera`\n",
    "\n",
    "### Learning Objectives\n",
    "1. understand how a number of different supervised learning algorithms elarn by estimating their parameters from data to make new predictions\n",
    "\n",
    "2. understand the strengths and weaknesses of particular supervised learning methods\n",
    "\n",
    "3. learn how to apply specific supervised machine learning algorithms in Python with scikit-learn\n",
    "\n",
    "4. learn about general principles of supervised machine learning, like overfitting and how to avoid it\n",
    "\n",
    "### Review of important terms\n",
    "1. feature representation\n",
    "    - convert an object into datasets that a computer understands\n",
    "2. data instances/samples/examples(X)\n",
    "    - one row of variables, or one representation of an object instance\n",
    "3. target value\n",
    "    - the label of an object made by human\n",
    "4. training and test sets\n",
    "    - training set / test set = 75 / 25\n",
    "5. model/estimator\n",
    "    - model fitting produces a 'trained model'\n",
    "    - training is the process of estimating model parameters\n",
    "6. evaluation method b \n",
    "\n",
    "### Classification and Regression\n",
    "1. Both classification and regression take a set of training instances and learn a mapping to a *target value*\n",
    "\n",
    "2. For classification, the target value is a discrete class value\n",
    "    - Binary: target value is 0 (negative class) or 1 (positive class)\n",
    "        - e.g. detecting a fraudulent credit card transaction\n",
    "    - Multi-class: target value if one of a set of discrete values\n",
    "        - e.g. labelling the type of fruit from physical attributes\n",
    "    - Multi-label: there are multiple target values (labels)\n",
    "        - e.g. labelling the topics discussed on a Web page\n",
    "3. For regression, the target value is *continuous* (floating point/real-valued)\n",
    "\n",
    "4. Looking at the target value's type will guide you on what supervised learning method to use.\n",
    "\n",
    "5. Many supervised learning methods have 'flavors' for both classificatinon and regression\n",
    "\n",
    "### Supervised learning methods: Overview\n",
    "1. Two simple but powerful prediction algorighms:\n",
    "    - K-nearest neighbors\n",
    "    - Linear model fit using least-quares\n",
    "2. These represent two complementary approaches to supervised learning:\n",
    "    - K-nearest neighbors makes few assumptions about the strucutre of the data and gives potentially accurate but sometimes unstable predictions (sensitive to small changes in the training data).\n",
    "    - Linear models make strong assumptions about the structure of the data and give stable but potentially inaccurate predictions.\n",
    "    \n",
    "#### What is a model?\n",
    "It's a specific mathematical or computational description that expresses the relationship between a set of input variables and one or more outcome variables that are being studied or predicted. In statistical terms the input variables are called independent variables and the outcome variables are termed dependent variables.\n",
    "\n",
    "In Machine Learning we sue the term features to refer to the input, or independent variables. And target value or target label to refer tot he output, dependent variables.\n",
    "\n",
    "Models can be either used to understand and explore the structure within a given dataset, aka unsupervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "### Generalization, Overfitting, and Underfitting\n",
    "1. `Generalization ability` refers to an algorithm's ability to give accurate predictions for new, previously unseen data.\n",
    "\n",
    "2. Assumptions:\n",
    "    - Future unseen data (test set) will have the same properties as the current training sets.\n",
    "    - Thus, models that are accurate on the training set are expected to be accurate on the test set.\n",
    "    - But that may not happen if the trained model is tuned too specifically to the training set.\n",
    "\n",
    "3. Models that are too complex for the amount of training data available are said to *overfit* and are not likely to generalize well to new examples.\n",
    "\n",
    "4. Models that are too simple, taht don't even do well on the training data, are said to *underfit* and also not likely to generalize well.\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/ef/71cece8042a8c604d9ebfd411737fa527f9d26.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/20/ccf12e2d6fe70ec1a680d0e94102cb739bb01a.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/48/6bfeb4b589147831425eb8f9799af8c87be47b.png\"> \n",
    "    - In K-Nearest Classification, when we decrese K, we increase the risk of overfitting because we're trying to capture very local changes in the decision boundary hat may not lead to good generalization behavior for future data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Datasets\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/89/b8b328ea5f2d46790f923929495eeee13513d5.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/15/7f3fd778c50a05cdbf0311ff81c8ba41dfbbc5.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/24/4130d8719b033a1b9b0143b5bca905dfec355c.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors: Classfication and Regression\n",
    "\n",
    "### k-Nearest neighbors classification\n",
    "<img src=\"https://img.ceclinux.org/ae/1ef55f55e051d656dfc8dd3496dc2b7c0620fa.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/d7/53bcfdea65a335cfe9ae3a0b5259dcdb9e7141.png\">\n",
    "    - The two exmaples above, when K increases, the accuracy in training data drops a bit but the accuracy in test data goes up a bit too, indicating the model is more effitive at ignoring minor variations in training data.\n",
    "    \n",
    "### k-Nearest neighbors regression\n",
    "<img src=\"https://img.ceclinux.org/ed/c6b9cb3e37b36bdfbbae2a43727caa4d601ee9.png\">\n",
    "\n",
    "The R-squared Regression\n",
    "<img src=\"https://img.ceclinux.org/09/b896c23b05e8b12f189fbaed3037d25ccbf12c.png\">\n",
    "\n",
    "1. Pros and Cons of nearest neighbor approach\n",
    "    - simple and easy to understand why a particular prediction is made\n",
    "    - could be a reasonble baseline against performance of more sophisticated models\n",
    "    - when training data has many instances, or each instance has lots of features, it slows down the performance of a k-nearest neighbors model\n",
    "    - so if your data set has hundreds or thousands of feature, esp. your data is sparse, you should consider other alternative models\n",
    "    \n",
    "### KNeighborsClassifier and KNeighborsRegressor: important parameters\n",
    "\n",
    "1. *Model complexity*\n",
    "    - n_neighbors: number of nearest neighbors(k) to consider \n",
    "        - default = 5\n",
    "2. *Model fittnig*\n",
    "    - metric: distance function between data points\n",
    "        - deault: Minkowsk distance with power parameter p=2 (Euclidean)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Least-Squares\n",
    "\n",
    "### Linear Models\n",
    "A linear model is a *sum of weighted variables* that predicts a target output value given an input data instance. E.g. predicting housing prices\n",
    "<img src=\"https://img.ceclinux.org/66/e788ce762b8ea553e8d77d524db933cd3359ea.png\">\n",
    "\n",
    "### Linear Regression is an Example of a Linear Model\n",
    "<img src=\"https://img.ceclinux.org/b8/db2255a79bcb9ea43e5026f1a0268350a21eb2.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/5c/3be4f292bd715fd18b7a43e1c99adea3766c06.png\">\n",
    "\n",
    "### Least-squares Linear Regression (\"Ordinary least-squares\")\n",
    "1. Finds w and b that minimizes the mean squared error of the model: the sum of squared differences between predicted target and actual target values (RSS), aka mean squared error of the linear model\n",
    "\n",
    "2. No parameters to control model complexity -- both pro and con\n",
    "<img src=\"https://img.ceclinux.org/4d/933ff29df41b17904125fe121fd6a21a12c6b8.png\">\n",
    "<img src=\"https://img.ceclinux.org/4b/8343b730d5ade002ca5e57a34c214cfba6cd29.png\">\n",
    "\n",
    "### How are Linear Regression Parameters *w*, *b* Estimated?\n",
    "1. Parameters are estimated from training data\n",
    "\n",
    "2. There are many different ways to estimate *w* and *b*:\n",
    "    - Different methods correspond to different \"fit\" criteria and goals and wyas to control model complexity\n",
    "3. The learning algorighm finds the parameters that optimize an `objective function`, typically to minimize some kind of `loss function` of the predicted target values vs. actual target values\n",
    "\n",
    "### Least-Squares Linear Regression in Sciki-Learn\n",
    "<img src=\"https://img.ceclinux.org/fe/cc874229fd0bb6a4ca9cbfd7b38f914610eb8f.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Ridge, Lasso, and Polynomial Regression\n",
    "\n",
    "### Ridge Regression\n",
    "1. Ridge regression learns *w*, *b* using the same least-squares criterion but adds a penalty for larget variations in *w* parameters\n",
    "    - $ RSS_{RIDGE}(w, b) = \\sum_{i=1}^{N} (y_i - (w \\times x_i + b))^2 + \\alpha \\sum_{j=1}^{p} w_j^2$\n",
    "2. Once the parameters are learned, the ridge regression **prediction** formula is the **same** as ordinary least-squares\n",
    "\n",
    "3. The addition of a parameter penalty is call **regularization**. Regularization prevents overfitting by restricting the model, typically to reduce its complexity.\n",
    "\n",
    "4. Ridge regress uses **L2 regularization**: minimize sum of squares of *w* entries\n",
    "\n",
    "5. The influence of the regularization term is controlled by the $\\alpha$ parameter. Default of $\\alpha$ is 1. Setting it to zero corresponds to ordinary least-squares linear regression\n",
    "\n",
    "6. Higher alpha means more regularization and simpler models\n",
    "\n",
    "### The Need for Feature Normalization\n",
    "1. Important for some machine learning methods taht all features are on the same scale (e.g. faster convergence in learning, more uniform or 'fair' in influence for all weights)\n",
    "    - e.g. regularzied regression, k-NN, support vector machines, neural networks, ...\n",
    "2. Can also depend on the data. More on feature engineering alter in the course. For now, we do MinMax scaling of the features:\n",
    "    - for each feature $x_i$: compute the min value $x-{i_MIN}$ and the max value $x-{i_MAX}$ achieved across all instances in the training set.\n",
    "    - for each feature: transform a given feature $x_i$ value to a scaled version $x_i^{'}$ using the formula\n",
    "\n",
    "### Feature Normalization: The test set must use identical scaling to the training set\n",
    "1. Fit the scaler using the training set, then apply the same scaler to transform the test set.\n",
    "\n",
    "2. Do not scale the training and test sets using different scalers: this could lead to random skew in the data\n",
    "\n",
    "3. Do not fit the scaler using any part of the test data: referencing the test data can lead to a form of *data leakage*\n",
    "\n",
    "*regularization works especially well when you have relatively small amounts of trainign data compared to the number of features in the model. Regularization becomes less important as the amount of training data increases.*\n",
    "\n",
    "### Lasso Regression: another form of regularized linear regression that uses and **L1 regularization** penalty for training (instead of Ridge's L2 penalty)\n",
    "1. L1 penalty: minimize the sum of the **absolute values** of the coefficients\n",
    "    - $ RSS_{LASSO}(w, b) = \\sum_{i=1}^{N} (y_i - (w \\times x_i + b))^2 + \\alpha \\sum_{j=1}^{p} \\left| w_j\\right |$\n",
    "2. This has the effect of setting parameter weights in *w* to **zero** for the least influential variables. This is called a **sparse** solution: a kind of feature selection\n",
    "\n",
    "3. The parameter $\\alpha$ controls amount of L1 regularization (default = 1.0)\n",
    "\n",
    "4. The prediction formula is the same as ordinary least-squares\n",
    "\n",
    "5. When to use Ridge or Lasso regression:\n",
    "    - many small/ medium sized effects: use Ridge\n",
    "    - Only a few variables with medium/ large effect: use lasso\n",
    "\n",
    "### Polynomial Features with Linear Regression\n",
    "$x= (x_0, x_1)$ --> $x^{'}= (x_0, x_1, x_0^2, x_0x_1, x_1^2)$\n",
    "\n",
    "$\\hat y = \\hat w_0x_0 + \\hat w_1x_1 + \\hat w_{00}x_0^2 + \\hat w_{01}x_0x_1 + \\hat w_{11}x_1^2 + b$\n",
    "\n",
    "1. Geneerate new features consisting of all polynomial combinations fo the original two features $(x_0, x_1)$\n",
    "\n",
    "2. the *degree* of the polynomial specifies how many variables participate at a time in each new feature (above example: degree2)\n",
    "\n",
    "3. This is still a weighted linear combination of features, so it's **still a linear model**, and can use smae least-squares estimation method for *w* and *b*\n",
    "\n",
    "<img serc=\"https://img.ceclinux.org/93/0715603fa87e8c4617a7449f8e316d3dbed129.png\">\n",
    "\n",
    "### Polynomial Features with Linear Regression\n",
    "1. Why would we want to transform our data this way?\n",
    "    - To capture interactions between the roiginal eatures by adding them as features to the linear model\n",
    "    - To make a classification problem easier\n",
    "2. More generally, we can apply other non-linear transformations to create new features\n",
    "    - Techically, these are called non-linear basis functions\n",
    "3. Beware of polynomial feature expansion with high degree, as this can lead to complex models that overfit\n",
    "    - Thus polynomial feature expansion is often combined with a regularzied learning method like Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
