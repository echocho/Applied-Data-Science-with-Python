{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Supervised Maching Learning\n",
    "`A more detailed and lower level ML course suggested: by Andrew Ng, also on Coursera`\n",
    "\n",
    "### Learning Objectives\n",
    "1. understand how a number of different supervised learning algorithms elarn by estimating their parameters from data to make new predictions\n",
    "\n",
    "2. understand the strengths and weaknesses of particular supervised learning methods\n",
    "\n",
    "3. learn how to apply specific supervised machine learning algorithms in Python with scikit-learn\n",
    "\n",
    "4. learn about general principles of supervised machine learning, like overfitting and how to avoid it\n",
    "\n",
    "### Review of important terms\n",
    "1. feature representation\n",
    "    - convert an object into datasets that a computer understands\n",
    "2. data instances/samples/examples(X)\n",
    "    - one row of variables, or one representation of an object instance\n",
    "3. target value\n",
    "    - the label of an object made by human\n",
    "4. training and test sets\n",
    "    - training set / test set = 75 / 25\n",
    "5. model/estimator\n",
    "    - model fitting produces a 'trained model'\n",
    "    - training is the process of estimating model parameters\n",
    "6. evaluation method b \n",
    "\n",
    "### Classification and Regression\n",
    "1. Both classification and regression take a set of training instances and learn a mapping to a *target value*\n",
    "\n",
    "2. For classification, the target value is a discrete class value\n",
    "    - Binary: target value is 0 (negative class) or 1 (positive class)\n",
    "        - e.g. detecting a fraudulent credit card transaction\n",
    "    - Multi-class: target value if one of a set of discrete values\n",
    "        - e.g. labelling the type of fruit from physical attributes\n",
    "    - Multi-label: there are multiple target values (labels)\n",
    "        - e.g. labelling the topics discussed on a Web page\n",
    "3. For regression, the target value is *continuous* (floating point/real-valued)\n",
    "\n",
    "4. Looking at the target value's type will guide you on what supervised learning method to use.\n",
    "\n",
    "5. Many supervised learning methods have 'flavors' for both classificatinon and regression\n",
    "\n",
    "### Supervised learning methods: Overview\n",
    "1. Two simple but powerful prediction algorighms:\n",
    "    - K-nearest neighbors\n",
    "    - Linear model fit using least-quares\n",
    "2. These represent two complementary approaches to supervised learning:\n",
    "    - K-nearest neighbors makes few assumptions about the strucutre of the data and gives potentially accurate but sometimes unstable predictions (sensitive to small changes in the training data).\n",
    "    - Linear models make strong assumptions about the structure of the data and give stable but potentially inaccurate predictions.\n",
    "    \n",
    "#### What is a model?\n",
    "It's a specific mathematical or computational description that expresses the relationship between a set of input variables and one or more outcome variables that are being studied or predicted. In statistical terms the input variables are called independent variables and the outcome variables are termed dependent variables.\n",
    "\n",
    "In Machine Learning we sue the term features to refer to the input, or independent variables. And target value or target label to refer tot he output, dependent variables.\n",
    "\n",
    "Models can be either used to understand and explore the structure within a given dataset, aka unsupervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "### Generalization, Overfitting, and Underfitting\n",
    "1. `Generalization ability` refers to an algorithm's ability to give accurate predictions for new, previously unseen data.\n",
    "\n",
    "2. Assumptions:\n",
    "    - Future unseen data (test set) will have the same properties as the current training sets.\n",
    "    - Thus, models that are accurate on the training set are expected to be accurate on the test set.\n",
    "    - But that may not happen if the trained model is tuned too specifically to the training set.\n",
    "\n",
    "3. Models that are too complex for the amount of training data available are said to *overfit* and are not likely to generalize well to new examples.\n",
    "\n",
    "4. Models that are too simple, taht don't even do well on the training data, are said to *underfit* and also not likely to generalize well.\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/ef/71cece8042a8c604d9ebfd411737fa527f9d26.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/20/ccf12e2d6fe70ec1a680d0e94102cb739bb01a.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/48/6bfeb4b589147831425eb8f9799af8c87be47b.png\"> \n",
    "    - In K-Nearest Classification, when we decrese K, we increase the risk of overfitting because we're trying to capture very local changes in the decision boundary hat may not lead to good generalization behavior for future data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Datasets\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/89/b8b328ea5f2d46790f923929495eeee13513d5.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/15/7f3fd778c50a05cdbf0311ff81c8ba41dfbbc5.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/24/4130d8719b033a1b9b0143b5bca905dfec355c.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors: Classfication and Regression\n",
    "\n",
    "### k-Nearest neighbors classification\n",
    "<img src=\"https://img.ceclinux.org/ae/1ef55f55e051d656dfc8dd3496dc2b7c0620fa.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/d7/53bcfdea65a335cfe9ae3a0b5259dcdb9e7141.png\">\n",
    "    - The two exmaples above, when K increases, the accuracy in training data drops a bit but the accuracy in test data goes up a bit too, indicating the model is more effitive at ignoring minor variations in training data.\n",
    "    \n",
    "### k-Nearest neighbors regression\n",
    "<img src=\"https://img.ceclinux.org/ed/c6b9cb3e37b36bdfbbae2a43727caa4d601ee9.png\">\n",
    "\n",
    "The R-squared Regression\n",
    "<img src=\"https://img.ceclinux.org/09/b896c23b05e8b12f189fbaed3037d25ccbf12c.png\">\n",
    "\n",
    "1. Pros and Cons of nearest neighbor approach\n",
    "    - simple and easy to understand why a particular prediction is made\n",
    "    - could be a reasonble baseline against performance of more sophisticated models\n",
    "    - when training data has many instances, or each instance has lots of features, it slows down the performance of a k-nearest neighbors model\n",
    "    - so if your data set has hundreds or thousands of feature, esp. your data is sparse, you should consider other alternative models\n",
    "    \n",
    "### KNeighborsClassifier and KNeighborsRegressor: important parameters\n",
    "\n",
    "1. *Model complexity*\n",
    "    - n_neighbors: number of nearest neighbors(k) to consider \n",
    "        - default = 5\n",
    "2. *Model fittnig*\n",
    "    - metric: distance function between data points\n",
    "        - deault: Minkowsk distance with power parameter p=2 (Euclidean)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
