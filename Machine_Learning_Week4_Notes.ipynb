{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Supervised Machine Learning - Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navie Bayes Classifiers\n",
    "\n",
    "### Naive Bayes Classifiers: a simple, probabilistic calssifier family\n",
    "1. These classifiers are called 'Naive' because they assume that features are conditionally independent, given the class.\n",
    "\n",
    "2. In other words: they assume that, for all instances of a given class, the features have little/no correlation with each other\n",
    "\n",
    "3. Hihgly efficient leanring and prediciton\n",
    "\n",
    "4. But generalization performance may worse than more sophisticated learning methods\n",
    "\n",
    "5. Can be competitive for some tasks, esp./usual. for high dimensional datasets\n",
    "\n",
    "### Naive Bayes Classifier Types\n",
    "1. Bernoulli: binary features (e.g. word presence/ absence)\n",
    "\n",
    "2. Multinomial: discrete features (e.g. word counts)\n",
    "\n",
    "3. Gaussian: continuous / real-valued features\n",
    "    - statistics computed for each class:\n",
    "        - for each feature: mean, standard deviation\n",
    "4. See the Applied Text Mining course for more details on the Bernoulli and Multinomial Naive Bayes models\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/85/09924c76f712eec77765916129aef4916f9c33.png\">\n",
    "<img src=\"https://img.ceclinux.org/2b/3de0dcee300c274f080636a7422c0c5a129914.png\">\n",
    "\n",
    "### Naive Bayes classifiers: Pros and Cons\n",
    "#### Pros:\n",
    "1. Easy to understand\n",
    "\n",
    "2. Simple, efficient parameter estimation\n",
    "\n",
    "3. Works well with high-dimensional data\n",
    "\n",
    "4. Often useful as a baseline comparision against more sophisticated methods\n",
    "\n",
    "#### Cons:\n",
    "1. Assumpition that features are conditionally independent given the class is not realistic\n",
    "\n",
    "2. As a result, other classifier types often have better generalization performance\n",
    "\n",
    "3. Their confidence estimates for predictions are not very accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "### Definition\n",
    "1. An ensemble of trees, not just one tree\n",
    "\n",
    "2. Widely used, very good results on many problems\n",
    "\n",
    "3. sklearn.ensemble module:\n",
    "    - classification: *RandomForestCalssifier*\n",
    "    - regression: *RandomForestRegressor*\n",
    "4. One decision --> prone to overfiting\n",
    "\n",
    "5. Many decision trees --> more stable, better generalization\n",
    "\n",
    "6. Ensemble of trees should be diverse: introduce random variation into tree-building\n",
    "\n",
    "\n",
    "### Random Forest Process\n",
    "<img src=\"https://img.ceclinux.org/0a/75b6f075d0c9f11c8ec3f6c74a3eadba0a1571.png\">\n",
    "<img src=\"https://img.ceclinux.org/4d/c31fd469f1893c5056b966538ce54213f068f6.png\">\n",
    "\n",
    "### Random Forest *max_features* Parameter\n",
    "1. Learning is quite sensitive to *max_features*\n",
    "\n",
    "2. Setting *max_features = 1* leads to forests with diverse, more complex trees\n",
    "\n",
    "3. Setting *max_features* = \\<close to number of features>\\ will lead to similar forests with simpler trees.\n",
    "\n",
    "### Prediction Using Random Forests\n",
    "1. Make a prediction for every tree in the forest\n",
    "\n",
    "2. Combine individual predictions\n",
    "    - regression: mean of individual tree predictions\n",
    "    - classification:\n",
    "        - each tree gives probability for each class\n",
    "        - probabilities averaged across trees\n",
    "        - predict the class with highest probability\n",
    "\n",
    "### Random Forest: Pros and Cons\n",
    "#### Pros:\n",
    "1. Widely used, excellent prediction performance on many problems\n",
    "\n",
    "2. Doesn't require careful normalization of features or extensive parameter tuning\n",
    "\n",
    "3. Easily parallelized across multiple CPUs\n",
    "\n",
    "#### Cons:\n",
    "1. The resulting models are often difficult for humans to interpret\n",
    "\n",
    "2. Like decision trees, random forests may not be a good choice for very high-demensional tasks (.eg. text classifiers) compared to fast, accurate linear models\n",
    "\n",
    "### Random Forests: RandomForestClassifier Key Parameters\n",
    "1. *n_estimators*: number of trees to use in ensemble (defualt: 10)\n",
    "    - should be larger for larget datasets to reduce overfitting (but uses more compuation)\n",
    "2. *max_features*: has a strong effect of performance. Infuences the diversity fo trees in the forest\n",
    "    - default works well in practice, but adjusting may lead to some further gains\n",
    "3. *max_depth*: controls the depth of each tree (default: None. splits until all leaves are prue)\n",
    "4. *n_jobs*: how many cores to use in parallel during training\n",
    "\n",
    "5. Choose a fixed setting for the random_state parameter if you need reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient Boosted Decision Trees\n",
    "<img src=\"https://img.ceclinux.org/2d/44e4056091261e434f6065d27fd71aca5637f1.png\">\n",
    "\n",
    "### GBDT: Pos and Cons\n",
    "#### Pros\n",
    "1. Often best off-the-shelf accuracy on many problems.\n",
    "2. Using model for prediction requires only modest memory and is fast.\n",
    "3. Doesn't require careful normalization of features to perform well.\n",
    "4. Like decision trees, handles a mixture of feature types.\n",
    "\n",
    "#### Cons\n",
    "1. Like random forests, the models are often difficult for humans to interpret.\n",
    "2. Requires careful runing of the learning rate and other parameters.\n",
    "3. Training can require significant computation.\n",
    "4. Like decision trees, not recommended for text classification and other problems with very high dimensional sparse features, for accuracy and computational cost reasons.\n",
    "\n",
    "### GBDT: GardientBoostingClassifier Key Parameters\n",
    "1. *n_estimators*: sets number fo small decision trees to use (week learners) in the ensemble.\n",
    "2. *learning_rate*: controls emphasis on fixing errors from previous iteration.\n",
    "3. *n_estimators* and *learning_rate* are often tuned together.\n",
    "4. *n_estimators* is adjusted first, to best exploit memory and CPUs during traiing, then other parameters. \n",
    "5. *max_depth* is typically set to a smally value (e.g. 3-5) for most applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "*Recommended course: Neural Networks for Machine Learning by a pioneer in this field, Jeff Hinton.*\n",
    "\n",
    "### Multi-layer Perceptron with One Hidden Layer\n",
    "(and tanh activation function)\n",
    "- a simple neural netwrok for regression. \n",
    "- also called MLP\n",
    "- also known as feed-forward neural networks\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/10/24e7d981597aa87226b118b71a0545f162742b.png\">\n",
    "($h_i = tanh(w_{0i}x_0 + w_{1i}x_1 + w_{2i}x_2 + w_{3i}x_3$)\n",
    "\n",
    "Each hidden unit in the hidden layer computes a nonlinear function of the weighted sums of the input features. \n",
    "\n",
    "### Activation Functions\n",
    "<img src=\"https://img.ceclinux.org/b1/9da153cf4ef2249cb27fcadf0cc10066f66936.png\">\n",
    "\n",
    "### A single hidden layer network using 1, 10, or 100 units\n",
    "<img src=\"https://img.ceclinux.org/fb/fe89b29c77c5ac13b4d766002afdc2f0031896.png\">\n",
    "\n",
    "### Multi-layer Perceptron with Two Hidden Layers\n",
    "<img src=\"https://img.ceclinux.org/28/3eb4cf61bcd2dc6442705c391cf9b0a84cf309.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/af/079a879b0afce85520935d73e7ec15fbff8d49.png\">\n",
    "\n",
    "### L2 Regularization with the Alpha Parameter\n",
    "<img src=\"https://img.ceclinux.org/c2/f005798edb7cbe24f80c47cf4cabe989673b16.png\">\n",
    "\n",
    "### Pros and Cons of Neural Networks\n",
    "#### Pros:\n",
    "1. They form the basis of state-of-the-art models and can be formed into advanced architectures that effectively capture complex features given enough data and computation.\n",
    "\n",
    "#### Cons:\n",
    "1. Larger, more complex models require significant training time, data and customization.\n",
    "2. Careful reprocessing of the data is needed.\n",
    "3. A good choice when the features are of similar types, but less so when features of very different types. \n",
    "\n",
    "### MLPClassifier and MLPRegressor: important Parameters\n",
    "1. *hidden_layer_sizes*: sets the number of hidden layers (number of elements in list), and number of hidden units per layer (each list element). Default: 100. \n",
    "2. *alpha*: controls weight on the regularization penalty that shrinks weight to zero. Default: alpha=0.00001.\n",
    "3. *activation*: ocntrols the nonlinear function used for the activation function, including: *'relu'* (default), *'logistic'*, *'tanh'*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "1. When the data you're using to train contains information about what you're trying to predict.\n",
    "2. Introducing information about the target during training that would not legitimately be available during actural use.\n",
    "3. Obvious examples:\n",
    "    - including the label to be predicted as a feature\n",
    "    - including test data with training data\n",
    "4. If your model performance is too good to be true, it probably is and likely due to 'giveaway' features.\n",
    "\n",
    "### More Subtle Examples of Data Leakage\n",
    "1. Prediction target: will users stay on a site, or leave?\n",
    "    - Giveaway feature: total session lenght, based on information about **future** page visits\n",
    "2. Predicitng if a user on a finanacial site is likely to open an acount\n",
    "    - An account number filed that's only filled in once the user does open an account\n",
    "3. Diagnostic test to predict a medical condition\n",
    "    - The existing paient dataset contains a binary variable taht heppens to mark whether they had surgery for that condition.\n",
    "    - Combinations of missing diagnosis codes that are not be available while the patient's condition was still being studied.\n",
    "    - The patient ID could contain information about specific diagnosis paths (e.g. for routine visit vs specialist).\n",
    "4. Any of these leaked features is highly preditive of the target, but not legitinately availabel at the time prediction needs to be done. \n",
    "\n",
    "### Other Examples of Data Leakage\n",
    "1. Leakage in training data:\n",
    "    - performing data propocessing using parameters or results from analyzing the entire dataset: normalizing and rescaling, detecting and removing outliers, estimating missing values, feature selection.\n",
    "    - time-series datasets: using records from the future when computing features for the current prediction.\n",
    "    - errors in data values/ gathering or missing variable indicators (e.g. the special value 999) can encode information about missing data that reveals information about the future.\n",
    "2. Leakage in features:\n",
    "    - removing varaibles that are not legitmate without also removing variables that encode the same or related infromation (e.g. diagnosis info may still exist in patient ID)\n",
    "    - reversing of intentional randomization or anonymization that reveals specific information about e.g. users not legitimately available in actual use.\n",
    "3. Any of the above could be present in any external data joined to the training set.\n",
    "\n",
    "### Detecting Data Leakage\n",
    "1. Before building the model\n",
    "    - exploratory data analysis to find surprises in the data \n",
    "    - are there features very highly correlated with the target value?\n",
    "2. After building the model\n",
    "    - look for surprising feature behavior in the fitted model: are there features with very high weights, or high information gain?\n",
    "    - simple rule-based models like decision trees can help with features like account numbers, patient IDs\n",
    "    - is overall model performance surprisingly good compared to known results on the same dataset, or for similar probelsm on similar datasets?\n",
    "3. Limited real-world deployment of the trained model\n",
    "    - potentially expensive in terms of developent time, but more realistic\n",
    "    - is this trained model generalizing well to new data?\n",
    "\n",
    "### Minimizing Data Leakage\n",
    "1. Performing data preparation within each cross-validation fold separately\n",
    "    - scale/ normalize data, performan feature selection, etc. within each fold separately, not using the entire dataset.\n",
    "    - for any such parameters estimated on the training data, you must use those same parameters to prepare data on the corresponding held-out test fold. \n",
    "2. With time series data, use a timestamp cutoff\n",
    "    - the cutoff value is set to the specific time point where prediction is to occur using current and past records\n",
    "    - using a curoff time will make sure you aren't accessing any data recors that were gathered after the prediction time, i.e. in the future\n",
    "3. Before any work with a new dataset, split off a final test validation dataset \n",
    "    - ... if you have enough data\n",
    "    - use this final test dataset as the very last step in your validation\n",
    "    - helps to check the true generalization perfromance of any trained models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "1. One of the key challenges in ML is finding the right features to use as input to a leanring model for a particular problem. That's calso called feature engineering. Sometimes finding is the right feature is more important than the choice of the model itself.\n",
    "2. Deep learning includes a sophisticated automatic eatured learning phase as part of its supervised training. And the feature extraction doesn't use just one feature leanring step, but a hierarchy of multiple feature learning layers. \n",
    "3. Starting from primitive, low-level features in the initial layer, each feature layer's output provides the input features to the next higher feature layer.\n",
    "4. All features are used in the final supervised learning model.\n",
    "\n",
    "### Example of a simple deep learning architecture\n",
    "<img src=\"https://img.ceclinux.org/2c/e9d8ef893838e2a4b0dc347c6dbf617cacc289.png\">\n",
    "\n",
    "### Another example\n",
    "<img src=\"https://img.ceclinux.org/30/0c054a907208f2867f386e5b5b68615718805c.png\">\n",
    "<img src=\"https://img.ceclinux.org/12/b23ec64215c931fadc01f5621f89218f808544.png\">\n",
    "\n",
    "### Pros and Cons of Deep Learning \n",
    "#### Pros:\n",
    "1. Powerful: deep learning has achieved significant gains over other ML approaches on many difficult learning tasks, leading to state-of-the-art performance across many different domains.\n",
    "2. Does effective automatic feature extraction, reducing the need for guesswork and heuristics on this key problem.\n",
    "3. current software provides flexible architectures that can be adapted for new domains airly easily.\n",
    "\n",
    "#### Cons:\n",
    "1. Can require huge amounts of training data and computing power.\n",
    "2. Architectures can be complex and often must be highly tailored to a specific application.\n",
    "\n",
    "### Deep Learning Software for Python\n",
    "1. Keras.\n",
    "2. Lasagne.\n",
    "3. TensorFlow.\n",
    "4. Theano.\n",
    "5. libraries support high-performance compuatation via GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Readings\n",
    "1. Carter, C., & Tanz, O. (2017, April 13). Neural networks made easy. Retrieved May 10, 2017, from https://techcrunch.com/2017/04/13/neural-networks-made-easy/.\n",
    "2.  Play with neural networks: http://playground.tensorflow.org/ .\n",
    "3.  High-level concepts of deep learning and reinforces the basic concepts covered in the Neural Networks and Deep Learning lectures: https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/ .\n",
    "4. An example of how deep learning is being used in healthcare: https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html .\n",
    "5. https://medium.com/@colin.fraser/the-treachery-of-leakage-56a2d7c4e931\n",
    "6. If you want an example in more depth of how data scientists are exploring ways to detect and avoid data leakage, this technical article proposes one approach: a two-stage process based on \"legitimacy tags\". If you're just interested in getting a little more background on the problem along with interesting examples, Sections 1 and 2 (Introduction and Related Work) are also useful to read on their own.Kaufman, S., Rosset, S., & Perlich, C. (2011). Leakage in data mining. Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11. doi:10.1145/2020408.2020496.\n",
    "7. Excellent example of how subtle or not-so-subtle leakage can occur in specific features: \n",
    "https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865#25839#post25839.\n",
    "8. This optional reading is intended mainly for software engineers who want to build and deploy machine learning applications in production - especially at scale. The only background knowledge required are the basic machine learning concepts we've covered so far in this course. Written by Google's Dr. Martin Zinkevich, it walks through a set of software engineering best practices for designing and deploying machine learning in software systems - based on years of practical experience at Google.\n",
    "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning\n",
    "\n",
    "## Introduction\n",
    "1. Unsupervised learning involves tasks that operate on datasets **without** labeled responses or target values.\n",
    "2. The goal is to capture interesting structure or information.\n",
    "3. Applications of unsupervised learning:\n",
    "    - visualize structure of a complex dataset\n",
    "    - density estimation to predict probabilities of events\n",
    "    - extract features for supervised learning\n",
    "    - discover important clusters or outliers\n",
    "<img src=\"https://img.ceclinux.org/86/4c8581a5fa80a1b10bc15e25e02455e4aa6bbc.png\">\n",
    "\n",
    "### Two Major Types of unsupervised Learning Methods\n",
    "1. Transformations\n",
    "    - processes taht extract or compute information\n",
    "2. Clustering\n",
    "    - find groups in the data\n",
    "    - assign every point in the dataset to one of the groups\n",
    "<img src= \"https://img.ceclinux.org/e3/6f5cc13145ac5adfe138b8394122728104c2d8.png\">\n",
    "<img src=\"https://img.ceclinux.org/84/de23c2c50ffaddb922fb8040112afcb199947f.png\">\n",
    "<img src=\"https://img.ceclinux.org/5e/22485960dd847d478626375204640d5462119e.png\">\n",
    "<img src=\"https://img.ceclinux.org/fd/ea9cea0f661c33d849f6f8134c231e62f605ba.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demensionality Reduction and Manifold Learning\n",
    "\n",
    "### Demensionality Reduction\n",
    "1. Finds an approximate version of your dataset using fewer features.\n",
    "2. Used for exploring and visualizing a dataset to understand grouping or relationships.\n",
    "3. Also used for compression, finding features for spervised learning.\n",
    "<img src=\"https://img.ceclinux.org/27/7b3dac8d6a64f27b194a1bf244c957e647e72f.png\">\n",
    "\n",
    "#### Pricipal Component Analysis (PCA)\n",
    "1. An important form of dimensionality reduction.\n",
    "2. What PCA does:\n",
    "    - Takes the could of original data points and finds a rotation of it. So the dimensions are statistically uncorrelated.\n",
    "    - And then drops all but the most informative initial dimensions that acapture most of the variation in the original dataset.\n",
    "<img src=\"https://img.ceclinux.org/3d/c330a9c3b7ede95b60a81b223643398f04b4fc.png\">\n",
    "<img src=\"https://img.ceclinux.org/d7/786698060004ff016633480a88d7f4a08a70d6.png\">\n",
    "\n",
    "### Manifold Learning\n",
    "1. Very good at finding low dimensional strucutre in high dimensional data and are very useful for visualizations.\n",
    "2. A classic example: the 'Swiss Roll' dataset.\n",
    "<img src=\"https://img.ceclinux.org/71/3520ac7735bb05bc29b5b26dec00414331614a.png\">\n",
    "-- the lower dimensional sheet within a higher dimensional space is called the manifold.\n",
    "\n",
    "#### Multi-dimensional Scaling (MDS)\n",
    "A widely used manifold learning method. \n",
    "<img src=\"https://img.ceclinux.org/be/b63a47ff1f73f47a886ba0ccbd5cf4adeb4d64.png\">\n",
    "\n",
    "#### t-SNE\n",
    "1. A powerful manifold leanring method that finds a 2D projection preserving information about neighbors.\n",
    "2. It finds a two-dimensional representation of your data, such that the distances between points in the 2D scatterplot match as closely as possible the distances between the same points in the original high dimensional dataset.\n",
    "3. it workds better on datasets that hae more well-defined local structure, or say, more clearly defined patterns of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "1. Finds a way to divide a dataset into groups ('cluster').\n",
    "2. Data points within the same cluster should be 'close' or 'similar' in some way. \n",
    "3. Data points in different clusters should be 'far apart' or 'different'.\n",
    "4. Clustering algorithms output a cluster membership inde for each data point:\n",
    "    - hard clustering: each data point belogns to exactly one cluster\n",
    "    - soft (fuzzy) clustering: each data point is assigned a weight, score, or probability of membership for each cluster\n",
    "\n",
    "### K-means Clustering \n",
    "1. initializaiton\n",
    "    - pick the number of clusters *k* you want to find (also a drawback of this algorithm, because sometimes we just don't know how many classes the original dataset should fall into in advance)\n",
    "    - pick *k random* pints to serve as an initial guess for the cluster centers\n",
    "2. step A\n",
    "    - assign each data point to the nearest cluster center.\n",
    "3. step B.\n",
    "    - update each cluster center by replacing it with the mean of all points assigned to that cluster (in step A).\n",
    "4. repeat steps A and B until the centers converge to a stable solution\n",
    "\n",
    "5. K-means is very sensitive to the range of feature values. Therefore, if your data has features with very different ranges, you need to normalize, using MinMaxScaling.\n",
    "\n",
    "#### Limitations of k-means\n",
    "1. Works well for simple clusters that are same size, well-separated, globular shapes.\n",
    "2. Does not do well with irregular, complex clusters.\n",
    "3. Variants of k-means like k-medoids can work with categorical features.\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/c9/7dd40f12423e80da92f8629cc9f87140396132.png\">\n",
    "\n",
    "### Linkage Criteria for Agglomerative Clustering\n",
    "1. Ward's method\n",
    "    - Least increase in total variance (around cluster centroids)\n",
    "2. Average linkage\n",
    "    - Average distance between clusters\n",
    "3. Complete linkage\n",
    "    - Max distance between clusters\n",
    "<img src=\"https://img.ceclinux.org/0f/3263d9ce56407b43c489b168b718794d5acb38.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/a7/171bfc077b1ff18308c77ce6cefcae711ed7af.png\">\n",
    "\n",
    "### DBSCAN Clustering\n",
    "0. Density-based Saptial Clustering of Applications with Noise.\n",
    "1. Unlike k-means, you don't need to speciy nubmer of clusters.\n",
    "2. Relatively efficient- can be used with large datasets.\n",
    "3. Ientifies likely noise points.\n",
    "<img src=\"https://img.ceclinux.org/9c/2fa5961c4a6bd7540558f5d34125db4e3d4a43.png\">\n",
    "\n",
    "4. Better if data ise scaled using a standard scalar or mi-max scalar to make sure the feature values have comparable ranges.\n",
    "\n",
    "5. When using the cluster assignments from DBSCAN, check for and handle the -1 noise value appropriately.\n",
    "\n",
    "### Cluster Evaluation\n",
    "1. With ground truth, existing labels can be used to evaluate cluster quality.\n",
    "2. Without ground truth, evaluation can be difficult: multiple clusterings may be plausible fro a dataset.\n",
    "3. Consider task-based evaluation: Evaluate clustering accoridng to performance on a task taht **does** have an objective basis for comparison.\n",
    "4. Example: the effectiveness of clustering-based features for a supervised learning task.\n",
    "5. Some evaluation heuristics exist (e.g. silhouette) but these can be unreliable.\n",
    "<img src=\"https://img.ceclinux.org/02/d4f8c64aee6509e3dc6edaf3c5e8a2a3d9b4a4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading:\n",
    "1. [How to use t-SNE effectively](http://distill.pub/2016/misread-tsne/#citation)\n",
    "2. [How Machines Make Sens of Big Data: an introduction to Clustering Algorithms](https://medium.freecodecamp.com/how-machines-make-sense-of-big-data-an-introduction-to-clustering-algorithms-4bd97d4fbaba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
