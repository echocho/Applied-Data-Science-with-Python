{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Supervised Machine Learning - Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navie Bayes Classifiers\n",
    "\n",
    "### Naive Bayes Classifiers: a simple, probabilistic calssifier family\n",
    "1. These classifiers are called 'Naive' because they assume that features are conditionally independent, given the class.\n",
    "\n",
    "2. In other words: they assume that, for all instances of a given class, the features have little/no correlation with each other\n",
    "\n",
    "3. Hihgly efficient leanring and prediciton\n",
    "\n",
    "4. But generalization performance may worse than more sophisticated learning methods\n",
    "\n",
    "5. Can be competitive for some tasks, esp./usual. for high dimensional datasets\n",
    "\n",
    "### Naive Bayes Classifier Types\n",
    "1. Bernoulli: binary features (e.g. word presence/ absence)\n",
    "\n",
    "2. Multinomial: discrete features (e.g. word counts)\n",
    "\n",
    "3. Gaussian: continuous / real-valued features\n",
    "    - statistics computed for each class:\n",
    "        - for each feature: mean, standard deviation\n",
    "4. See the Applied Text Mining course for more details on the Bernoulli and Multinomial Naive Bayes models\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/85/09924c76f712eec77765916129aef4916f9c33.png\">\n",
    "<img src=\"https://img.ceclinux.org/2b/3de0dcee300c274f080636a7422c0c5a129914.png\">\n",
    "\n",
    "### Naive Bayes classifiers: Pros and Cons\n",
    "#### Pros:\n",
    "1. Easy to understand\n",
    "\n",
    "2. Simple, efficient parameter estimation\n",
    "\n",
    "3. Works well with high-dimensional data\n",
    "\n",
    "4. Often useful as a baseline comparision against more sophisticated methods\n",
    "\n",
    "#### Cons:\n",
    "1. Assumpition that features are conditionally independent given the class is not realistic\n",
    "\n",
    "2. As a result, other classifier types often have better generalization performance\n",
    "\n",
    "3. Their confidence estimates for predictions are not very accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "### Definition\n",
    "1. An ensemble of trees, not just one tree\n",
    "\n",
    "2. Widely used, very good results on many problems\n",
    "\n",
    "3. sklearn.ensemble module:\n",
    "    - classification: *RandomForestCalssifier*\n",
    "    - regression: *RandomForestRegressor*\n",
    "4. One decision --> prone to overfiting\n",
    "\n",
    "5. Many decision trees --> more stable, better generalization\n",
    "\n",
    "6. Ensemble of trees should be diverse: introduce random variation into tree-building\n",
    "\n",
    "\n",
    "### Random Forest Process\n",
    "<img src=\"https://img.ceclinux.org/0a/75b6f075d0c9f11c8ec3f6c74a3eadba0a1571.png\">\n",
    "<img src=\"https://img.ceclinux.org/4d/c31fd469f1893c5056b966538ce54213f068f6.png\">\n",
    "\n",
    "### Random Forest *max_features* Parameter\n",
    "1. Learning is quite sensitive to *max_features*\n",
    "\n",
    "2. Setting *max_features = 1* leads to forests with diverse, more complex trees\n",
    "\n",
    "3. Setting *max_features* = \\<close to number of features>\\ will lead to similar forests with simpler trees.\n",
    "\n",
    "### Prediction Using Random Forests\n",
    "1. Make a prediction for every tree in the forest\n",
    "\n",
    "2. Combine individual predictions\n",
    "    - regression: mean of individual tree predictions\n",
    "    - classification:\n",
    "        - each tree gives probability for each class\n",
    "        - probabilities averaged across trees\n",
    "        - predict the class with highest probability\n",
    "\n",
    "### Random Forest: Pros and Cons\n",
    "#### Pros:\n",
    "1. Widely used, excellent prediction performance on many problems\n",
    "\n",
    "2. Doesn't require careful normalization of features or extensive parameter tuning\n",
    "\n",
    "3. Easily parallelized across multiple CPUs\n",
    "\n",
    "#### Cons:\n",
    "1. The resulting models are often difficult for humans to interpret\n",
    "\n",
    "2. Like decision trees, random forests may not be a good choice for very high-demensional tasks (.eg. text classifiers) compared to fast, accurate linear models\n",
    "\n",
    "### Random Forests: RandomForestClassifier Key Parameters\n",
    "1. *n_estimators*: number of trees to use in ensemble (defualt: 10)\n",
    "    - should be larger for larget datasets to reduce overfitting (but uses more compuation)\n",
    "2. *max_features*: has a strong effect of performance. Infuences the diversity fo trees in the forest\n",
    "    - default works well in practice, but adjusting may lead to some further gains\n",
    "3. *max_depth*: controls the depth of each tree (default: None. splits until all leaves are prue)\n",
    "4. *n_jobs*: how many cores to use in parallel during training\n",
    "\n",
    "5. Choose a fixed setting for the random_state parameter if you need reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient Boosted Decision Trees\n",
    "<img src=\"https://img.ceclinux.org/2d/44e4056091261e434f6065d27fd71aca5637f1.png\">\n",
    "\n",
    "### GBDT: Pos and Cons\n",
    "#### Pros\n",
    "1. Often best off-the-shelf accuracy on many problems.\n",
    "2. Using model for prediction requires only modest memory and is fast.\n",
    "3. Doesn't require careful normalization of features to perform well.\n",
    "4. Like decision trees, handles a mixture of feature types.\n",
    "\n",
    "#### Cons\n",
    "1. Like random forests, the models are often difficult for humans to interpret.\n",
    "2. Requires careful runing of the learning rate and other parameters.\n",
    "3. Training can require significant computation.\n",
    "4. Like decision trees, not recommended for text classification and other problems with very high dimensional sparse features, for accuracy and computational cost reasons.\n",
    "\n",
    "### GBDT: GardientBoostingClassifier Key Parameters\n",
    "1. *n_estimators*: sets number fo small decision trees to use (week learners) in the ensemble.\n",
    "2. *learning_rate*: controls emphasis on fixing errors from previous iteration.\n",
    "3. *n_estimators* and *learning_rate* are often tuned together.\n",
    "4. *n_estimators* is adjusted first, to best exploit memory and CPUs during traiing, then other parameters. \n",
    "5. *max_depth* is typically set to a smally value (e.g. 3-5) for most applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "*Recommended course: Neural Networks for Machine Learning by a pioneer in this field, Jeff Hinton.*\n",
    "\n",
    "### Multi-layer Perceptron with One Hidden Layer\n",
    "(and tanh activation function)\n",
    "- a simple neural netwrok for regression. \n",
    "- also called MLP\n",
    "- also known as feed-forward neural networks\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/10/24e7d981597aa87226b118b71a0545f162742b.png\">\n",
    "($h_i = tanh(w_{0i}x_0 + w_{1i}x_1 + w_{2i}x_2 + w_{3i}x_3$)\n",
    "\n",
    "Each hidden unit in the hidden layer computes a nonlinear function of the weighted sums of the input features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
