{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Supervised Machine Learning - Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navie Bayes Classifiers\n",
    "\n",
    "### Naive Bayes Classifiers: a simple, probabilistic calssifier family\n",
    "1. These classifiers are called 'Naive' because they assume that features are conditionally independent, given the class.\n",
    "\n",
    "2. In other words: they assume that, for all instances of a given class, the features have little/no correlation with each other\n",
    "\n",
    "3. Hihgly efficient leanring and prediciton\n",
    "\n",
    "4. But generalization performance may worse than more sophisticated learning methods\n",
    "\n",
    "5. Can be competitive for some tasks, esp./usual. for high dimensional datasets\n",
    "\n",
    "### Naive Bayes Classifier Types\n",
    "1. Bernoulli: binary features (e.g. word presence/ absence)\n",
    "\n",
    "2. Multinomial: discrete features (e.g. word counts)\n",
    "\n",
    "3. Gaussian: continuous / real-valued features\n",
    "    - statistics computed for each class:\n",
    "        - for each feature: mean, standard deviation\n",
    "4. See the Applied Text Mining course for more details on the Bernoulli and Multinomial Naive Bayes models\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/85/09924c76f712eec77765916129aef4916f9c33.png\">\n",
    "<img src=\"https://img.ceclinux.org/2b/3de0dcee300c274f080636a7422c0c5a129914.png\">\n",
    "\n",
    "### Naive Bayes classifiers: Pros and Cons\n",
    "#### Pros:\n",
    "1. Easy to understand\n",
    "\n",
    "2. Simple, efficient parameter estimation\n",
    "\n",
    "3. Works well with high-dimensional data\n",
    "\n",
    "4. Often useful as a baseline comparision against more sophisticated methods\n",
    "\n",
    "#### Cons:\n",
    "1. Assumpition that features are conditionally independent given the class is not realistic\n",
    "\n",
    "2. As a result, other classifier types often have better generalization performance\n",
    "\n",
    "3. Their confidence estimates for predictions are not very accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "### Definition\n",
    "1. An ensemble of trees, not just one tree\n",
    "\n",
    "2. Widely used, very good results on many problems\n",
    "\n",
    "3. sklearn.ensemble module:\n",
    "    - classification: *RandomForestCalssifier*\n",
    "    - regression: *RandomForestRegressor*\n",
    "4. One decision --> prone to overfiting\n",
    "\n",
    "5. Many decision trees --> more stable, better generalization\n",
    "\n",
    "6. Ensemble of trees should be diverse: introduce random variation into tree-building\n",
    "\n",
    "\n",
    "### Random Forest Process\n",
    "<img src=\"https://img.ceclinux.org/0a/75b6f075d0c9f11c8ec3f6c74a3eadba0a1571.png\">\n",
    "<img src=\"https://img.ceclinux.org/4d/c31fd469f1893c5056b966538ce54213f068f6.png\">\n",
    "\n",
    "### Random Forest *max_features* Parameter\n",
    "1. Learning is quite sensitive to *max_features*\n",
    "\n",
    "2. Setting *max_features = 1* leads to forests with diverse, more complex trees\n",
    "\n",
    "3. Setting *max_features* = \\<close to number of features>\\ will lead to similar forests with simpler trees.\n",
    "\n",
    "### Prediction Using Random Forests\n",
    "1. Make a prediction for every tree in the forest\n",
    "\n",
    "2. Combine individual predictions\n",
    "    - regression: mean of individual tree predictions\n",
    "    - classification:\n",
    "        - each tree gives probability for each class\n",
    "        - probabilities averaged across trees\n",
    "        - predict the class with highest probability\n",
    "\n",
    "### Random Forest: Pros and Cons\n",
    "#### Pros:\n",
    "1. Widely used, excellent prediction performance on many problems\n",
    "\n",
    "2. Doesn't require careful normalization of features or extensive parameter tuning\n",
    "\n",
    "3. Easily parallelized across multiple CPUs\n",
    "\n",
    "#### Cons:\n",
    "1. The resulting models are often difficult for humans to interpret\n",
    "\n",
    "2. Like decision trees, random forests may not be a good choice for very high-demensional tasks (.eg. text classifiers) compared to fast, accurate linear models\n",
    "\n",
    "### Random Forests: RandomForestClassifier Key Parameters\n",
    "1. *n_estimators*: number of trees to use in ensemble (defualt: 10)\n",
    "    - should be larger for larget datasets to reduce overfitting (but uses more compuation)\n",
    "2. *max_features*: has a strong effect of performance. Infuences the diversity fo trees in the forest\n",
    "    - default works well in practice, but adjusting may lead to some further gains\n",
    "3. *max_depth*: controls the depth of each tree (default: None. splits until all leaves are prue)\n",
    "4. *n_jobs*: how many cores to use in parallel during training\n",
    "\n",
    "5. Choose a fixed setting for the random_state parameter if you need reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient Boosted Decision Trees\n",
    "<img src=\"https://img.ceclinux.org/2d/44e4056091261e434f6065d27fd71aca5637f1.png\">\n",
    "\n",
    "### GBDT: Pos and Cons\n",
    "#### Pros\n",
    "1. Often best off-the-shelf accuracy on many problems.\n",
    "2. Using model for prediction requires only modest memory and is fast.\n",
    "3. Doesn't require careful normalization of features to perform well.\n",
    "4. Like decision trees, handles a mixture of feature types.\n",
    "\n",
    "#### Cons\n",
    "1. Like random forests, the models are often difficult for humans to interpret.\n",
    "2. Requires careful runing of the learning rate and other parameters.\n",
    "3. Training can require significant computation.\n",
    "4. Like decision trees, not recommended for text classification and other problems with very high dimensional sparse features, for accuracy and computational cost reasons.\n",
    "\n",
    "### GBDT: GardientBoostingClassifier Key Parameters\n",
    "1. *n_estimators*: sets number fo small decision trees to use (week learners) in the ensemble.\n",
    "2. *learning_rate*: controls emphasis on fixing errors from previous iteration.\n",
    "3. *n_estimators* and *learning_rate* are often tuned together.\n",
    "4. *n_estimators* is adjusted first, to best exploit memory and CPUs during traiing, then other parameters. \n",
    "5. *max_depth* is typically set to a smally value (e.g. 3-5) for most applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "*Recommended course: Neural Networks for Machine Learning by a pioneer in this field, Jeff Hinton.*\n",
    "\n",
    "### Multi-layer Perceptron with One Hidden Layer\n",
    "(and tanh activation function)\n",
    "- a simple neural netwrok for regression. \n",
    "- also called MLP\n",
    "- also known as feed-forward neural networks\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/10/24e7d981597aa87226b118b71a0545f162742b.png\">\n",
    "($h_i = tanh(w_{0i}x_0 + w_{1i}x_1 + w_{2i}x_2 + w_{3i}x_3$)\n",
    "\n",
    "Each hidden unit in the hidden layer computes a nonlinear function of the weighted sums of the input features. \n",
    "\n",
    "### Activation Functions\n",
    "<img src=\"https://img.ceclinux.org/b1/9da153cf4ef2249cb27fcadf0cc10066f66936.png\">\n",
    "\n",
    "### A single hidden layer network using 1, 10, or 100 units\n",
    "<img src=\"https://img.ceclinux.org/fb/fe89b29c77c5ac13b4d766002afdc2f0031896.png\">\n",
    "\n",
    "### Multi-layer Perceptron with Two Hidden Layers\n",
    "<img src=\"https://img.ceclinux.org/28/3eb4cf61bcd2dc6442705c391cf9b0a84cf309.png\">\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/af/079a879b0afce85520935d73e7ec15fbff8d49.png\">\n",
    "\n",
    "### L2 Regularization with the Alpha Parameter\n",
    "<img src=\"https://img.ceclinux.org/c2/f005798edb7cbe24f80c47cf4cabe989673b16.png\">\n",
    "\n",
    "### Pros and Cons of Neural Networks\n",
    "#### Pros:\n",
    "1. They form the basis of state-of-the-art models and can be formed into advanced architectures that effectively capture complex features given enough data and computation.\n",
    "\n",
    "#### Cons:\n",
    "1. Larger, more complex models require significant training time, data and customization.\n",
    "2. Careful reprocessing of the data is needed.\n",
    "3. A good choice when the features are of similar types, but less so when features of very different types. \n",
    "\n",
    "### MLPClassifier and MLPRegressor: important Parameters\n",
    "1. *hidden_layer_sizes*: sets the number of hidden layers (number of elements in list), and number of hidden units per layer (each list element). Default: 100. \n",
    "2. *alpha*: controls weight on the regularization penalty that shrinks weight to zero. Default: alpha=0.00001.\n",
    "3. *activation*: ocntrols the nonlinear function used for the activation function, including: *'relu'* (default), *'logistic'*, *'tanh'*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "1. One of the key challenges in ML is finding the right features to use as input to a leanring model for a particular problem. That's calso called feature engineering. Sometimes finding is the right feature is more important than the choice of the model itself.\n",
    "2. Deep learning includes a sophisticated automatic eatured learning phase as part of its supervised training. And the feature extraction doesn't use just one feature leanring step, but a hierarchy of multiple feature learning layers. \n",
    "3. Starting from primitive, low-level features in the initial layer, each feature layer's output provides the input features to the next higher feature layer.\n",
    "4. All features are used in the final supervised learning model.\n",
    "\n",
    "### Example of a simple deep learning architecture\n",
    "<img src=\"https://img.ceclinux.org/2c/e9d8ef893838e2a4b0dc347c6dbf617cacc289.png\">\n",
    "\n",
    "### Another example\n",
    "<img src=\"https://img.ceclinux.org/30/0c054a907208f2867f386e5b5b68615718805c.png\">\n",
    "<img src=\"https://img.ceclinux.org/12/b23ec64215c931fadc01f5621f89218f808544.png\">\n",
    "\n",
    "### Pros and Cons of Deep Learning \n",
    "#### Pros:\n",
    "1. Powerful: deep learning has achieved significant gains over other ML approaches on many difficult learning tasks, leading to state-of-the-art performance across many different domains.\n",
    "2. Does effective automatic feature extraction, reducing the need for guesswork and heuristics on this key problem.\n",
    "3. current software provides flexible architectures that can be adapted for new domains airly easily.\n",
    "\n",
    "#### Cons:\n",
    "1. Can require huge amounts of training data and computing power.\n",
    "2. Architectures can be complex and often must be highly tailored to a specific application.\n",
    "\n",
    "### Deep Learning Software for Python\n",
    "1. Keras.\n",
    "2. Lasagne.\n",
    "3. TensorFlow.\n",
    "4. Theano.\n",
    "5. libraries support high-performance compuatation via GPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Readings\n",
    "1. Carter, C., & Tanz, O. (2017, April 13). Neural networks made easy. Retrieved May 10, 2017, from https://techcrunch.com/2017/04/13/neural-networks-made-easy/.\n",
    "2.  Play with neural networks: http://playground.tensorflow.org/ .\n",
    "3.  High-level concepts of deep learning and reinforces the basic concepts covered in the Neural Networks and Deep Learning lectures: https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/ .\n",
    "4. An example of how deep learning is being used in healthcare: https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html .\n",
    "5. https://medium.com/@colin.fraser/the-treachery-of-leakage-56a2d7c4e931\n",
    "6. If you want an example in more depth of how data scientists are exploring ways to detect and avoid data leakage, this technical article proposes one approach: a two-stage process based on \"legitimacy tags\". If you're just interested in getting a little more background on the problem along with interesting examples, Sections 1 and 2 (Introduction and Related Work) are also useful to read on their own.Kaufman, S., Rosset, S., & Perlich, C. (2011). Leakage in data mining. Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11. doi:10.1145/2020408.2020496.\n",
    "7. Excellent example of how subtle or not-so-subtle leakage can occur in specific features: \n",
    "https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865#25839#post25839.\n",
    "8. This optional reading is intended mainly for software engineers who want to build and deploy machine learning applications in production - especially at scale. The only background knowledge required are the basic machine learning concepts we've covered so far in this course. Written by Google's Dr. Martin Zinkevich, it walks through a set of software engineering best practices for designing and deploying machine learning in software systems - based on years of practical experience at Google.\n",
    "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
