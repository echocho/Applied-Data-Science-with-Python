{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation & Selection\n",
    "### Learning Objectives \n",
    "1. Understand why accuracy only gives a partial picture of a classifier's performance.\n",
    "\n",
    "2. Understand the motivation and definition of important evaluation metrics in machine learning.\n",
    "\n",
    "3. Learn how to use a variety of evalution metrics to evaluate supervised machine learning models.\n",
    "\n",
    "4. Learn about choosing the right metric for selecting between models or for doing parameter tuning.\n",
    "\n",
    "### About Evaluation\n",
    "1. Different applications have very different goals.\n",
    "\n",
    "2. Accuracy is widely used, but many others are possible, e.g.\n",
    "    - user satisfaction (web search)\n",
    "    - amount of revenue (e-commerce)\n",
    "    - increase in patient survival rates (medical)\n",
    "\n",
    "3. It's very important to choose evaluation methods that match the goal of your application.\n",
    "\n",
    "4. Compute your selected evaluation metric for multiple different models.\n",
    "\n",
    "5. Then select the model with 'best' value of evaluation metric\n",
    "\n",
    "### Accuracy with Imbalanced Classes\n",
    "1. Suppose you have two classes:\n",
    "    - Relevant (R): the positive class\n",
    "    - Not_Relevant (N): the negative class\n",
    "2. Out of 1000 randomly selected items, on average \n",
    "    - One item is relevant and has an R label\n",
    "    - The rest of the items (999 of them) are not relevant and labelled N.\n",
    "3. Recall that:\n",
    "    Accuracy = **#correct predictions / # total instances**\n",
    "4. You build a classifier to predict relevant items, and see that its accuracy on a test set is 99.9%. (You may think: wow, this is amazing. But wait..)\n",
    "\n",
    "5. For comparison, suppose we had a 'dummy' classifier that didn't look at the features at all, and always just blindly predicted the most frequent class (i.e. the negative N class)\n",
    "\n",
    "6. Assuming a test set of 1000 instances, what would this dummy classifier's accuracy be?\n",
    "    - $Accuracy_{DUMMY} = 999/1000 = 99.9%$\n",
    "\n",
    "### Dummy classifiers completely ignore the input data\n",
    "1. dumm yclassifiers serve as a sanity check on your classifier's performance\n",
    "\n",
    "2. They provide a *null metric* (e.g. null accuracy) baseline\n",
    "\n",
    "3. Dummy classifiers should not be used for real problems\n",
    "\n",
    "4. Som commonly-used settings for the strategy parameter for DummyClassifier in scikit-learn:\n",
    "    - most_frequent: predicts the most frequent label in the training set\n",
    "    - stratified: random preditions based on training set class distribution\n",
    "    - unifrom: generates predictions uniformly at random\n",
    "    - constant: always predicts a constant label provided by the user\n",
    "        - a majormotivation of this method is F1-scoring, when the positive class is in the minority\n",
    "        \n",
    "### What if my classifier accuracy is close to the null accuracy baseline?\n",
    "This could be a sign of:\n",
    "1. Ineffective, erroneous or missing features\n",
    "\n",
    "2. Poor choice of kernel or hyperparameter\n",
    "\n",
    "3. Large class imbalance\n",
    "\n",
    "### Dummy Regressors\n",
    "*strategy* parameter options:\n",
    "1. mean: predicts the mean of the training targets\n",
    "\n",
    "2. median: predicts the median of the training targets\n",
    "\n",
    "3. quantile: predicts a user-provided quantile of the training targets\n",
    "\n",
    "4. constant: predicts a constant user-provided value\n",
    "\n",
    "### Binary Prediction Outcomes \n",
    "<img src=\"https://img.ceclinux.org/6c/34448493ba9f254a61091449ba6f77c530231f.png\">\n",
    "\n",
    "### Confusion Matrix for Binary Prediction Task\n",
    "<img src=\"https://img.ceclinux.org/18/7739eb732a66ee82dd5f6c78d1e33a2ab7828a.png\">\n",
    "\n",
    "** Noete: Always look at the confusion matrix for your classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices & Basic Evaluation Metrics\n",
    "\n",
    "### **Accuracy**: for what fraction of all instances is the classifier's prediction correct (for either positive or negative class)?\n",
    "<img src=\"https://img.ceclinux.org/9a/80004a130a4e73aad41819829f140cf7721c59.png\">\n",
    "\n",
    "### **Classification Error (I - Accuracy)**: for what fraction of all instances is the classifier's prediciton *incorrect*?\n",
    "$ClassificationError = \\frac {FP + FN}{TN + TP + FN + FP} = \\frac{7+17}{400+26+17+7} = 0.060$\n",
    "\n",
    "### **Recall**, or **True Positive Rate (TPR)**:  what fraction of all positive instances does the classifier *correctly* indentify as positive?\n",
    "Recall is also known as:\n",
    "    - True Positive Rate (TPR)\n",
    "    - Sensitivity \n",
    "    - Probability of detection\n",
    "\n",
    "$Recall = \\frac {TP} {TP + FN} = \\frac {26} {26 + 17} = 0.60$\n",
    "\n",
    "### **Precision**: what fraction of *positive* predictions are correct?\n",
    "$Precision = \\frac {TP}{TP + FP} = \\frac {26}{26 + 7} = 0.79$\n",
    "\n",
    "### **False posivite rate (FPR)**: what fraction of all negative instances does the classifier *incorrectly* identify as positive?\n",
    "False Positive Rate is also known as: Specificity\n",
    "\n",
    "$FPR = \\frac {FP}{TN + FP} = \\frac {7}{400+7} = 0.02$\n",
    "\n",
    "### The Precision-Recall Tradeoff\n",
    "<img src=\"https://img.ceclinux.org/c5/0a5988cacc5b7c718b9dae42e7cf4e04ca99b0.png\">\n",
    "<img src=\"https://img.ceclinux.org/f0/4165bf69694b90c80bf4ebacb7f0fb91547fd5.png\">\n",
    "<img src=\"https://img.ceclinux.org/ad/9e1952c54a420b374e73d0b5177f4afe961708.png\">\n",
    "\n",
    "### There is often a tradeoff between precision and recall\n",
    "1. Recall-oriented machine learning tasks:\n",
    "    - Search and information extraction in legal discovery\n",
    "    - Tumor detection\n",
    "    - Often paired with a human export to filter out false positives\n",
    "2. Precision-oriented machine learning tasks:\n",
    "    - Search engine ranking, query suggestion\n",
    "    - Documentation classification\n",
    "    -  Many cusotmer-facing tasks (users remeber failures!)\n",
    "    \n",
    "### F-score: generalizes F1-score for combining precision & recall into a single number\n",
    "<img src=\"https://img.ceclinux.org/e7/2dca94ed7c096a518ef78ec8f079499ffcf1db.png\">\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, or \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Decision Functions\n",
    "\n",
    "### Decision Functions (decision_function)\n",
    "1. Each classifier score value per test point indicates how confidently the classifier predicts the positive class (large-magnitude positive values) or the negative class (large-magnitude negative values)\n",
    "\n",
    "2. Choosing a fixed decision threshold gives a classifiction rule.\n",
    "\n",
    "3. By sweeping the decision threshold through the entire range of possible score values, we get a series of classification outcomes that form a curve.\n",
    "\n",
    "### Predicted Probability of Class Membership (predict_proba)\n",
    "1. Typical rule: choose most likely class\n",
    "    - e.g. class I if threshold > 0.50\n",
    "2. Adjusting threshold afects predictions of classifier\n",
    "\n",
    "3. Higher threshold results in a more conservative classifier\n",
    "    - e.g. only predict Class I if estimated probability of class I is above 70%\n",
    "    - this increases precision. Doesn't predict class I as often, but when it dows, it gets high proportion of class I instances correct\n",
    "4. Not all models provide realistic probability estimates\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/66/45dcd40322e36e7c00f3fda299e06f3072782b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall and ROC Curves\n",
    "\n",
    "### Precision-Recall Curves\n",
    "1. X-axis: Precision\n",
    "\n",
    "2. Y-axis: Recall\n",
    "\n",
    "3. Top right corner:\n",
    "    - the 'ideal' point\n",
    "    - precision = 1.0\n",
    "    - recall = 1.0\n",
    "\n",
    "4. \"Steepness\" of P-R curves is important:\n",
    "    - maximize precision\n",
    "    - while maximizing recall\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/93/89e371aa62b37ddd90cfff2eafd36eccf00f47.png\">\n",
    "\n",
    "### ROC Curves\n",
    "Receiver operating characteristic curve\n",
    "\n",
    "1. X-axis: False Positive Rate\n",
    "\n",
    "2. Y-axis: True Positive Rate\n",
    "\n",
    "3. Top left corner:\n",
    "    - the 'ideal' point\n",
    "    - false positive rate of zero\n",
    "    - true positive rate of one\n",
    "\n",
    "4. 'Steepness' of ROC curves is important\n",
    "    - maximize the true positive rate\n",
    "    - while minimizing the false positive rate\n",
    "\n",
    "<img src=\"https://img.ceclinux.org/a1/c32857808150f646216766ee6140a016a54159.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Evaluation\n",
    "1. Multi-class evaluation is an extension of the binary case\n",
    "    - A collection of true vs predicted binary outcomes, one per class\n",
    "    - Confusion matrices are especially useful\n",
    "    - Classification report\n",
    "2. Overall evaluation metrics are averages across classes\n",
    "    - But there are different ways to average multi-class results\n",
    "    - The support (number of instances) for each class is important to consider, e.g. in case of imbalanced classes\n",
    "3. Multi-label classification: each instance can have multiple labels (not covered here)\n",
    "\n",
    "### Multi-Class Confusion Matrix\n",
    "<img src=\"https://img.ceclinux.org/3d/1b9658381d93761e91f336afd1dfea2b764d50.png\">\n",
    "**should look at the confusion matrix of each of your model, to gain useful insights**\n",
    "\n",
    "### Micro vs Macro Average\n",
    "#### Macro-average\n",
    "1. Each **class** has equal weight\n",
    "\n",
    "2. Compute metric within each class\n",
    "\n",
    "3. Average resulting metrics across classes\n",
    "<img src=\"https://img.ceclinux.org/39/52703afcf6ee0c3c4834dc99af01fc655f6230.png\">\n",
    "\n",
    "#### Micro-average\n",
    "1. Each **instance** has equal weight\n",
    "\n",
    "2. Largest classes have most influence\n",
    "\n",
    "3. Aggregate outcomes across all classes\n",
    "\n",
    "4. Compute metric with aggregate outcomes\n",
    "<img src=\"https://img.ceclinux.org/d7/f36d014927a1762df9658cfda98732732ddaea.png\">\n",
    "\n",
    "### Macro-Average vs Micro-Average\n",
    "1. If the calsses have about the same number of instances, macro- and micro-average will be about the same\n",
    "\n",
    "2. If some classes are much larger (more instances) than others, and you want to:\n",
    "    - weight your metric toward the largetst ones, use micro-averaging\n",
    "    - weight your metric toward the smallest ones, use macro-averaging\n",
    "3. If the micro-average is much lower than the macro-average then examine the larger classes for poor metric performance\n",
    "\n",
    "4. If the macro-average is much lower than the micro-average then examine the smaller classes for poor metric performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Evaluation\n",
    "1. Typically r2 is enough\n",
    "    - Reminder: computes how well future instances will be predicted\n",
    "    - Best possible score is 1.0\n",
    "    - Constant prediction score is 0.0\n",
    "2. Alternative metrics include:\n",
    "    - mean_absolute_error (absolute difference of target & predicted values); corresponds to the expected value of the L1 norm loss\n",
    "    - mean_squared_error (squared difference of target & predicted values); corresponds to the expected value of the L2 norm loss\n",
    "    - median_absolute_error (robust to outliers)\n",
    "    \n",
    "### Dummy Regressors\n",
    "As in classification, comparison to a 'dummy' prediction model that uses a fixed rule can be useful. *cummy regressors* in scikit-learn\n",
    "\n",
    "The DummyRegressor class implements 4 simple baseline rules for regression, using the **strategy** parameter:\n",
    "    - **mean** predicts the mean of the training target values\n",
    "    - **median** predicts the median of the training target values\n",
    "    - **quantile** predicts a user-provided quantile fo the training target values (e.g. value at the 75th percentile)\n",
    "    - **constant** predicts a custom constant value provided by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Optimizing Classifiers for Different Evaluation Metrics\n",
    "\n",
    "### Model Selection Using Evaluation Metrics\n",
    "1. Train/test on same data\n",
    "    - single metric\n",
    "    - typically overfits and likely won't generalize well to new data\n",
    "    - but can serves as a sanity check: low accuracy on the training set may indicate an implementation problem\n",
    "2. single train/test split\n",
    "    - single metric\n",
    "    - speed and simplicity\n",
    "    - lack of variance information\n",
    "3. K-fold cross-validation\n",
    "    - K train-test splits\n",
    "    - Average metric over all splits\n",
    "    - Can be combined with parameter grid search: GridSearchCV (def. cv=3)\n",
    "\n",
    "### Training, Validation, and Test Framework for Model Selection and Evaluation\n",
    "1. Using ony corss-validation or a test set to do model selection may lead to more subtle overfitting/ optimistic generalization estimates\n",
    "\n",
    "2. Instead, use three data splits:\n",
    "    - training set (model building)\n",
    "    - validation set (model selection)\n",
    "    - test set (final evaluation)\n",
    "3. In practice:\n",
    "    - create an initial training/ test split\n",
    "    - do cross-validation on the training data for model/parameter selection\n",
    "    - save the held-out teset set fo rfinal model evaluation\n",
    "\n",
    "### Concluding Notes\n",
    "1. Accuracy is often not the right evaluation metric for many real-world machine learning tasks\n",
    "    - False positives and false negatives may need to be treated very differently\n",
    "    - Make sure you understand the needs of your application and choose an evaluation metric that matches your application, user, or business goals\n",
    "2. Examples of additional evaluation methods include:\n",
    "    - Learning curve: How much does accuracy (or other metric) change as a funciton of the amount of training data?\n",
    "    - Sensitivity analysis: How much does accuracy (or other metric) change as a function of key learning parameter values?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
