{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation & Selection\n",
    "### Learning Objectives \n",
    "1. Understand why accuracy only gives a partial picture of a classifier's performance.\n",
    "\n",
    "2. Understand the motivation and definition of important evaluation metrics in machine learning.\n",
    "\n",
    "3. Learn how to use a variety of evalution metrics to evaluate supervised machine learning models.\n",
    "\n",
    "4. Learn about choosing the right metric for selecting between models or for doing parameter tuning.\n",
    "\n",
    "### About Evaluation\n",
    "1. Different applications have very different goals.\n",
    "\n",
    "2. Accuracy is widely used, but many others are possible, e.g.\n",
    "    - user satisfaction (web search)\n",
    "    - amount of revenue (e-commerce)\n",
    "    - increase in patient survival rates (medical)\n",
    "\n",
    "3. It's very important to choose evaluation methods that match the goal of your application.\n",
    "\n",
    "4. Compute your selected evaluation metric for multiple different models.\n",
    "\n",
    "5. Then select the model with 'best' value of evaluation metric\n",
    "\n",
    "### Accuracy with Imbalanced Classes\n",
    "1. Suppose you have two classes:\n",
    "    - Relevant (R): the positive class\n",
    "    - Not_Relevant (N): the negative class\n",
    "2. Out of 1000 randomly selected items, on average \n",
    "    - One item is relevant and has an R label\n",
    "    - The rest of the items (999 of them) are not relevant and labelled N.\n",
    "3. Recall that:\n",
    "    Accuracy = **#correct predictions / # total instances**\n",
    "4. You build a classifier to predict relevant items, and see that its accuracy on a test set is 99.9%. (You may think: wow, this is amazing. But wait..)\n",
    "\n",
    "5. For comparison, suppose we had a 'dummy' classifier that didn't look at the features at all, and always just blindly predicted the most frequent class (i.e. the negative N class)\n",
    "\n",
    "6. Assuming a test set of 1000 instances, what would this dummy classifier's accuracy be?\n",
    "    - $Accuracy_{DUMMY} = 999/1000 = 99.9%$\n",
    "\n",
    "### Dummy classifiers completely ignore the input data\n",
    "1. dumm yclassifiers serve as a sanity check on your classifier's performance\n",
    "\n",
    "2. They provide a *null metric* (e.g. null accuracy) baseline\n",
    "\n",
    "3. Dummy classifiers should not be used for real problems\n",
    "\n",
    "4. Som commonly-used settings for the strategy parameter for DummyClassifier in scikit-learn:\n",
    "    - most_frequent: predicts the most frequent label in the training set\n",
    "    - stratified: random preditions based on training set class distribution\n",
    "    - unifrom: generates predictions uniformly at random\n",
    "    - constant: always predicts a constant label provided by the user\n",
    "        - a majormotivation of this method is F1-scoring, when the positive class is in the minority\n",
    "        \n",
    "### What if my classifier accuracy is close to the null accuracy baseline?\n",
    "This could be a sign of:\n",
    "1. Ineffective, erroneous or missing features\n",
    "\n",
    "2. Poor choice of kernel or hyperparameter\n",
    "\n",
    "3. Large class imbalance\n",
    "\n",
    "### Dummy Regressors\n",
    "*strategy* parameter options:\n",
    "1. mean: predicts the mean of the training targets\n",
    "\n",
    "2. median: predicts the median of the training targets\n",
    "\n",
    "3. quantile: predicts a user-provided quantile of the training targets\n",
    "\n",
    "4. constant: predicts a constant user-provided value\n",
    "\n",
    "### Binary Prediction Outcomes \n",
    "<img src=\"https://img.ceclinux.org/6c/34448493ba9f254a61091449ba6f77c530231f.png\">\n",
    "\n",
    "### Confusion Matrix for Binary Prediction Task\n",
    "<img src=\"https://img.ceclinux.org/18/7739eb732a66ee82dd5f6c78d1e33a2ab7828a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
